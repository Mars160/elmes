#!/usr/bin/env python3
"""
è¯„ä¼°å™¨æµ‹è¯•è„šæœ¬
æµ‹è¯•è¯„ä¼°å™¨çš„å„ç§åŠŸèƒ½
"""

from src.evaluator.base import Evaluator
import glob
import os

def test_single_file_evaluation():
    """æµ‹è¯•å•ä¸ªæ–‡ä»¶è¯„ä¼°"""
    print("=" * 60)
    print("ğŸ§ª æµ‹è¯•å•ä¸ªæ–‡ä»¶è¯„ä¼°")
    print("=" * 60)
    
    # åˆ›å»ºè¯„ä¼°å™¨ï¼ˆä½¿ç”¨Ollamaï¼‰
    evaluator = Evaluator(evaluation_backend_config="openai")
    
    # åˆå§‹åŒ–
    success = evaluator.initialize()
    if not success:
        print("âŒ è¯„ä¼°å™¨åˆå§‹åŒ–å¤±è´¥")
        return
    
    # æ‰¾åˆ°ä¸€ä¸ªæµ‹è¯•æ–‡ä»¶
    result_files = glob.glob("results/*.json")
    if not result_files:
        print("âš ï¸  æœªæ‰¾åˆ°ç»“æœæ–‡ä»¶")
        return
    
    test_file = result_files[0]
    print(f"ğŸ“ æµ‹è¯•æ–‡ä»¶: {test_file}")
    
    try:
        # è¯„ä¼°æ–‡ä»¶
        eval_result = evaluator.evaluate_result_file(
            test_file,
            Student_image="å°å­¦äºŒå¹´çº§å­¦ç”Ÿï¼Œå¥³ç”Ÿï¼Œå¾ˆå–œæ¬¢æ•°å­¦ï¼Œå¯¹æ–°çŸ¥è¯†å……æ»¡å¥½å¥‡",
            Student_query="ä¸‹èŠ‚è¯¾å­¦ä¹ æœ‰ç†æ•°ï¼Œä½ å¸®æˆ‘é¢„ä¹ "
        )
        print(f"âœ… è¯„ä¼°æˆåŠŸ: {eval_result}")
        
        # æ˜¾ç¤ºè¯„ä¼°ç»“æœæ‘˜è¦
        with open(eval_result, 'r', encoding='utf-8') as f:
            import json
            result = json.load(f)
            print(f"ğŸ“Š è¯„ä¼°æ‘˜è¦:")
            print(f"   - åŸå§‹æ–‡ä»¶: {result['original_file']}")
            print(f"   - åœºæ™¯: {result['scenario']}")
            print(f"   - ä»»åŠ¡ID: {result['task_id']}")
            print(f"   - è¯„ä¼°æ¨¡å‹: {result['evaluation_model']}")
            print(f"   - è¯„ä¼°æ—¶é—´: {result['evaluation_timestamp']}")
            
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")

def test_batch_evaluation():
    """æµ‹è¯•æ‰¹é‡è¯„ä¼°"""
    print("\n" + "=" * 60)
    print("ğŸš€ æµ‹è¯•æ‰¹é‡è¯„ä¼°")
    print("=" * 60)
    
    # åˆ›å»ºè¯„ä¼°å™¨ï¼ˆä½¿ç”¨OpenAIï¼Œå¦‚æœé…ç½®äº†çš„è¯ï¼‰
    evaluator = Evaluator(evaluation_backend_config="ollama")  # å…ˆç”¨ollamaæµ‹è¯•
    
    # åˆå§‹åŒ–
    success = evaluator.initialize()
    if not success:
        print("âŒ è¯„ä¼°å™¨åˆå§‹åŒ–å¤±è´¥")
        return
    
    try:
        # æ‰¹é‡è¯„ä¼°ï¼ˆé™åˆ¶ä¸ºå‰3ä¸ªæ–‡ä»¶ï¼‰
        pattern = "results/*.json"
        result_files = glob.glob(pattern)[:3]  # åªè¯„ä¼°å‰3ä¸ªæ–‡ä»¶
        
        if not result_files:
            print("âš ï¸  æœªæ‰¾åˆ°ç»“æœæ–‡ä»¶")
            return
        
        print(f"ğŸ“ å‡†å¤‡è¯„ä¼° {len(result_files)} ä¸ªæ–‡ä»¶")
        
        # æ‰‹åŠ¨æ‰¹é‡è¯„ä¼°ä»¥ä¾¿æ·»åŠ å‚æ•°
        evaluation_files = []
        for result_file in result_files:
            try:
                eval_file = evaluator.evaluate_result_file(
                    result_file,
                    Student_image="å°å­¦äºŒå¹´çº§å­¦ç”Ÿï¼Œå¥³ç”Ÿï¼Œå¾ˆå–œæ¬¢æ•°å­¦",
                    Student_query="å­¦ä¹ ç›¸å…³çŸ¥è¯†ç‚¹"
                )
                evaluation_files.append(eval_file)
                print(f"   âœ… {os.path.basename(result_file)} -> {os.path.basename(eval_file)}")
            except Exception as e:
                print(f"   âŒ {os.path.basename(result_file)}: {e}")
        
        print(f"ğŸ‰ æ‰¹é‡è¯„ä¼°å®Œæˆï¼ŒæˆåŠŸè¯„ä¼° {len(evaluation_files)} ä¸ªæ–‡ä»¶")
        
        # æ˜¾ç¤ºè¯„ä¼°ç»“æœç›®å½•
        print(f"ğŸ“‚ è¯„ä¼°ç»“æœä¿å­˜åœ¨: evaluation_result/")
        eval_files = glob.glob("evaluation_result/*.json")
        print(f"ğŸ“Š æ€»è®¡ {len(eval_files)} ä¸ªè¯„ä¼°ç»“æœæ–‡ä»¶")
        
    except Exception as e:
        print(f"âŒ æ‰¹é‡è¯„ä¼°å¤±è´¥: {e}")

def test_different_backends():
    """æµ‹è¯•ä¸åŒçš„åç«¯"""
    print("\n" + "=" * 60)
    print("ğŸ”„ æµ‹è¯•ä¸åŒè¯„ä¼°åç«¯")
    print("=" * 60)
    
    backends = ["ollama"]  # å…ˆåªæµ‹è¯•ollama
    
    # æ‰¾ä¸€ä¸ªæµ‹è¯•æ–‡ä»¶
    result_files = glob.glob("results/*.json")
    if not result_files:
        print("âš ï¸  æœªæ‰¾åˆ°ç»“æœæ–‡ä»¶")
        return
    
    test_file = result_files[0]
    
    for backend in backends:
        print(f"\nğŸ§ª æµ‹è¯•åç«¯: {backend}")
        try:
            evaluator = Evaluator(evaluation_backend_config=backend)
            success = evaluator.initialize()
            
            if success:
                eval_file = evaluator.evaluate_result_file(
                    test_file,
                    Student_image="å°å­¦ç”Ÿï¼Œå¯¹æ•°å­¦æ„Ÿå…´è¶£",
                    Student_query="å­¦ä¹ æ•°å­¦çŸ¥è¯†"
                )
                print(f"   âœ… {backend} åç«¯è¯„ä¼°æˆåŠŸ: {os.path.basename(eval_file)}")
            else:
                print(f"   âŒ {backend} åç«¯åˆå§‹åŒ–å¤±è´¥")
                
        except Exception as e:
            print(f"   âŒ {backend} åç«¯æµ‹è¯•å¤±è´¥: {e}")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ¯ è¯„ä¼°å™¨åŠŸèƒ½æµ‹è¯•å¼€å§‹")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs("evaluation_result", exist_ok=True)
    
    # æµ‹è¯•å•ä¸ªæ–‡ä»¶è¯„ä¼°
    test_single_file_evaluation()
    
    # æµ‹è¯•æ‰¹é‡è¯„ä¼°
    test_batch_evaluation()
    
    # æµ‹è¯•ä¸åŒåç«¯
    test_different_backends()
    
    print("\n" + "=" * 60)
    print("ğŸ è¯„ä¼°å™¨æµ‹è¯•å®Œæˆ")
    print("=" * 60)

if __name__ == "__main__":
    main() 