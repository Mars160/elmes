import os
import json
import yaml
import time
from datetime import datetime
from typing import Dict, Any, Optional
from pathlib import Path

from src.backend import OllamaBackend, OpenAIBackend, DeepSeekBackend


class Evaluator:
    """
    è¯„ä¼°å™¨ï¼Œç”¨äºå¯¹LLMç”Ÿæˆçš„æ•™å­¦å†…å®¹è¿›è¡Œè´¨é‡è¯„ä¼°
    
    ä¸»è¦åŠŸèƒ½ï¼š
    1. è§£æç»“æœæ–‡ä»¶ï¼Œæå–åœºæ™¯ã€é—®é¢˜å’Œå›ç­”
    2. æ ¹æ®åœºæ™¯åŠ è½½å¯¹åº”çš„è¯„ä¼°æç¤ºè¯
    3. è°ƒç”¨è¯„ä¼°æ¨¡å‹è¿›è¡Œè´¨é‡è¯„ä¼°
    4. ä¿å­˜è¯„ä¼°ç»“æœ
    """
    
    def __init__(self, backend_config: str = "openai", output_dir: str = "evaluation_result"):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            backend_config: åç«¯é…ç½®åç§° (openai, ollama, deepseek)
            output_dir: è¯„ä¼°ç»“æœè¾“å‡ºç›®å½•
        """
        self.backend_config = backend_config
        self.output_dir = output_dir
        self.backend = None
        self.evaluation_prompts = {}
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    def initialize(self) -> bool:
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Returns:
            bool: åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
        """
        try:
            self._load_evaluation_prompts()
            self._initialize_backend()
            return True
        except Exception as e:
            print(f"è¯„ä¼°å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def _load_evaluation_prompts(self):
        """åŠ è½½è¯„ä¼°æç¤ºè¯é…ç½®"""
        prompts_path = Path("config/prompts/evaluation_prompts.yaml")
        
        if not prompts_path.exists():
            raise FileNotFoundError(f"è¯„ä¼°æç¤ºè¯æ–‡ä»¶ä¸å­˜åœ¨: {prompts_path}")
        
        with open(prompts_path, 'r', encoding='utf-8') as f:
            self.evaluation_prompts = yaml.safe_load(f)
    
    def _initialize_backend(self):
        """åˆå§‹åŒ–åç«¯"""
        config = self._load_backend_config()
        
        # åˆ›å»ºåç«¯å®ä¾‹
        backend_map = {
            "openai": OpenAIBackend,
            "ollama": OllamaBackend,
            "deepseek": DeepSeekBackend
        }
        
        if self.backend_config not in backend_map:
            raise ValueError(f"ä¸æ”¯æŒçš„åç«¯é…ç½®: {self.backend_config}")
        
        backend_class = backend_map[self.backend_config]
        self.backend = backend_class(
            model_name=config.get("model_name"),
            api_base=config.get("api_base"),
            api_key=config.get("api_key")
        )
        
        if not self.backend.initialize():
            raise Exception(f"åç«¯ {self.backend_config} åˆå§‹åŒ–å¤±è´¥")
    
    def _load_backend_config(self) -> Dict[str, Any]:
        """åŠ è½½åç«¯é…ç½®"""
        config_path = Path("config/evaluation_backend.yaml")
        
        if not config_path.exists():
            raise FileNotFoundError(f"åç«¯é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        if self.backend_config not in config:
            raise ValueError(f"åç«¯é…ç½® '{self.backend_config}' ä¸å­˜åœ¨")
        
        return config[self.backend_config]
    
    def evaluate_file(self, result_file_path: str, **kwargs) -> str:
        """
        è¯„ä¼°ç»“æœæ–‡ä»¶
        
        Args:
            result_file_path: ç»“æœæ–‡ä»¶è·¯å¾„
            **kwargs: æ ¼å¼åŒ–å‚æ•°
            
        Returns:
            str: è¯„ä¼°ç»“æœæ–‡ä»¶è·¯å¾„
        """
        # 1. è§£æç»“æœæ–‡ä»¶
        result_data = self._load_result_file(result_file_path)
        scenario = result_data.get("scenario", "unknown")
        
        # 2. æå–Q&Aå†…å®¹
        user_content = self._extract_user_content(result_data)
        assistant_content = self._extract_assistant_content(result_data)
        
        # 3. æ„å»ºè¯„ä¼°æç¤ºè¯
        evaluation_prompt = self._build_evaluation_prompt(
            scenario, user_content, assistant_content, **kwargs
        )
        
        # 4. è°ƒç”¨è¯„ä¼°æ¨¡å‹
        evaluation_response = self._call_evaluation_model(evaluation_prompt)
        
        # 5. ä¿å­˜è¯„ä¼°ç»“æœ
        return self._save_evaluation_result(
            result_data, evaluation_response, result_file_path
        )
    
    def _load_result_file(self, file_path: str) -> Dict[str, Any]:
        """è¯»å–ç»“æœæ–‡ä»¶"""
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _extract_user_content(self, result_data: Dict[str, Any]) -> str:
        """æå–ç”¨æˆ·é—®é¢˜å†…å®¹"""
        for message in result_data.get("messages", []):
            if message.get("role") == "user":
                return message.get("content", "")
        return ""
    
    def _extract_assistant_content(self, result_data: Dict[str, Any]) -> str:
        """æå–åŠ©æ‰‹å›ç­”å†…å®¹"""
        raw_response = result_data.get("raw_response", {})
        choices = raw_response.get("choices", [])
        if choices:
            return choices[0].get("message", {}).get("content", "")
        return ""
    
    def _build_evaluation_prompt(self, scenario: str, user_content: str, 
                               assistant_content: str, **kwargs) -> str:
        """æ„å»ºè¯„ä¼°æç¤ºè¯"""
        if scenario not in self.evaluation_prompts:
            raise ValueError(f"åœºæ™¯ '{scenario}' çš„è¯„ä¼°æç¤ºè¯ä¸å­˜åœ¨")
        
        template = self.evaluation_prompts[scenario]
        
        # æ ¼å¼åŒ–å‚æ•°
        format_params = {
            "prompt": user_content,
            "query": assistant_content,
            "LLM_response": assistant_content,
            "Student_image": kwargs.get("Student_image", "å°å­¦ç”Ÿ"),
            "Student_query": kwargs.get("Student_query", user_content),
            **kwargs
        }
        
        # æ ¼å¼åŒ–æ¨¡æ¿
        try:
            return template.format(**format_params)
        except KeyError:
            # å¦‚æœæ ¼å¼åŒ–å¤±è´¥ï¼Œé€ä¸ªæ›¿æ¢å‚æ•°
            result = template
            for key, value in format_params.items():
                result = result.replace(f"{{{key}}}", str(value))
            return result
    
    def _call_evaluation_model(self, evaluation_content: str) -> Dict[str, Any]:
        """è°ƒç”¨è¯„ä¼°æ¨¡å‹"""
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•™å­¦è¯„ä¼°ä¸“å®¶ï¼Œè¯·æ ¹æ®ç»™å®šçš„æ ‡å‡†å¯¹æ•™å­¦å†…å®¹è¿›è¡Œå®¢è§‚ã€å…¬æ­£çš„è¯„ä¼°ã€‚"},
            {"role": "user", "content": evaluation_content}
        ]
        
        return self.backend.chat(
            messages=messages,
            temperature=0.1,
            max_tokens=2000
        )
    
    def _save_evaluation_result(self, result_data: Dict[str, Any], 
                              evaluation_response: Dict[str, Any], 
                              original_file_path: str) -> str:
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        # ç”Ÿæˆè¯„ä¼°ç»“æœæ–‡ä»¶å
        original_filename = Path(original_file_path).name
        evaluation_filename = f"eval_{original_filename}"
        evaluation_filepath = Path(self.output_dir) / evaluation_filename
        
        # æ„å»ºè¯„ä¼°ç»“æœæ•°æ®
        evaluation_result = {
            "original_file": original_file_path,
            "scenario": result_data.get("scenario"),
            "task_id": result_data.get("task_id"),
            "evaluation_backend": self.backend_config,
            "evaluation_model": getattr(self.backend, 'model_name', 'unknown'),
            "evaluation_response": evaluation_response,
            "evaluation_timestamp": datetime.now().isoformat()
        }
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        with open(evaluation_filepath, 'w', encoding='utf-8') as f:
            json.dump(evaluation_result, f, ensure_ascii=False, indent=2)
        
        return str(evaluation_filepath)


# ä¾¿æ·å‡½æ•°æ¥å£
def evaluate_json(result_file_path: str,
                  backend: str = "openai",
                  output_dir: str = "evaluation_result",
                  **format_kwargs) -> str:
    """
    è¯„ä¼°å•ä¸ªJSONç»“æœæ–‡ä»¶çš„ä¾¿æ·å‡½æ•°
    
    Args:
        result_file_path: ç»“æœJSONæ–‡ä»¶è·¯å¾„
        backend: åç«¯åç§° (openai, ollama, deepseek)
        output_dir: è¯„ä¼°ç»“æœè¾“å‡ºç›®å½•
        **format_kwargs: æ ¼å¼åŒ–å‚æ•° (å¦‚Student_image, Student_query)
        
    Returns:
        str: ä¿å­˜çš„è¯„ä¼°ç»“æœæ–‡ä»¶è·¯å¾„
    """
    evaluator = Evaluator(backend_config=backend, output_dir=output_dir)
    
    if not evaluator.initialize():
        raise RuntimeError("è¯„ä¼°å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥åç«¯é…ç½®")
    
    return evaluator.evaluate_file(result_file_path, **format_kwargs)


if __name__ == "__main__":
    import glob
    import argparse
    
    parser = argparse.ArgumentParser(description="è¯„ä¼°å•ä¸ªç»“æœJSONæ–‡ä»¶")
    parser.add_argument("json_path", nargs="?", help="JSONæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--backend", default="openai", help="åç«¯: openai, ollama, deepseek")
    args = parser.parse_args()
    
    if args.json_path:
        target_file = args.json_path
    else:
        files = glob.glob("results/*.json")
        if not files:
            print("âš ï¸  æœªæ‰¾åˆ°ç»“æœæ–‡ä»¶")
            exit(1)
        target_file = files[0]
    
    print(f"ğŸ“ è¯„ä¼°æ–‡ä»¶: {target_file}")
    
    try:
        saved_path = evaluate_json(target_file, backend=args.backend)
        print(f"âœ… è¯„ä¼°å®Œæˆï¼Œç»“æœä¿å­˜è‡³: {saved_path}")
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        exit(1) 


