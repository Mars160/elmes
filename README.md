# æ•™è‚²LLMè¯„ä¼°ç³»ç»Ÿ (**E**ducational L**LM** **E**valuation **S**ystem)

## ğŸ“– é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªå®Œæ•´çš„æ•™è‚²é¢†åŸŸå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¯„ä¼°ç³»ç»Ÿï¼Œç”¨äºè¯„ä¼°LLMåœ¨æ•™è‚²åœºæ™¯ä¸‹çš„è¡¨ç°è´¨é‡ã€‚ç³»ç»Ÿæ”¯æŒå¤šç§åç«¯æ¨¡å‹ï¼ˆOpenAIã€Ollamaã€DeepSeekï¼‰ï¼Œå¯ä»¥è‡ªåŠ¨åŒ–æ‰§è¡Œæ•™å­¦ä»»åŠ¡ï¼Œå¹¶å¯¹ç»“æœè¿›è¡Œä¸“ä¸šçš„æ•™å­¦è´¨é‡è¯„ä¼°ã€‚

## ğŸ—ï¸ é¡¹ç›®ç»“æ„

```
edu_evaluation/
â”œâ”€â”€ README.md                      # é¡¹ç›®è¯´æ˜æ–‡æ¡£
â”œâ”€â”€ requirements.txt               # Pythonä¾èµ–åŒ…åˆ—è¡¨
â”œâ”€â”€ setup.py                      # é¡¹ç›®å®‰è£…é…ç½®
â”œâ”€â”€ init.py                       # é¡¹ç›®åˆå§‹åŒ–è„šæœ¬
â”œâ”€â”€ 
â”œâ”€â”€ config/                       # é…ç½®æ–‡ä»¶ç›®å½•
â”‚   â”œâ”€â”€ config.yaml              # ä¸»é…ç½®æ–‡ä»¶ï¼ˆåœºæ™¯é…ç½®ï¼‰
â”‚   â”œâ”€â”€ test_backend.yaml        # æµ‹è¯•åç«¯é…ç½®
â”‚   â”œâ”€â”€ evaluation_backend.yaml  # è¯„ä¼°åç«¯é…ç½®
â”‚   â””â”€â”€ prompts/                 # æç¤ºè¯é…ç½®
â”‚       â””â”€â”€ evaluation_prompts.yaml  # è¯„ä¼°æç¤ºè¯
â”‚
â”œâ”€â”€ src/                         # æºä»£ç ç›®å½•
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ backend/                 # åç«¯æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py             # åç«¯åŸºç±»
â”‚   â”‚   â”œâ”€â”€ ollama_backend.py   # Ollamaåç«¯
â”‚   â”‚   â”œâ”€â”€ openai_backend.py   # OpenAIåç«¯
â”‚   â”‚   â””â”€â”€ deepseek_backend.py # DeepSeekåç«¯
â”‚   â”‚
â”‚   â”œâ”€â”€ executor/               # æ‰§è¡Œå™¨æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py            # æ‰§è¡Œå™¨åŸºç±»
â”‚   â”‚   â”œâ”€â”€ single_turn_executor.py  # å•è½®å¯¹è¯æ‰§è¡Œå™¨
â”‚   â”‚   â””â”€â”€ multi_turn_executor.py   # å¤šè½®å¯¹è¯æ‰§è¡Œå™¨
â”‚   â”‚
â”‚   â”œâ”€â”€ prompts/               # æç¤ºè¯ç”Ÿæˆå™¨
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py           # æç¤ºè¯åŸºç±»
â”‚   â”‚   â””â”€â”€ knowledge_explanation.py  # çŸ¥è¯†è§£é‡Šåœºæ™¯
â”‚   â”‚
â”‚   â””â”€â”€ evaluator/            # è¯„ä¼°å™¨æ¨¡å—
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ base.py          # è¯„ä¼°å™¨æ ¸å¿ƒåŠŸèƒ½
â”‚       â””â”€â”€ show_score.py    # åˆ†æ•°æ˜¾ç¤ºå·¥å…·
â”‚
â”œâ”€â”€ results/                  # æ‰§è¡Œç»“æœç›®å½•
â”‚   â””â”€â”€ *.json               # æ ¼å¼ï¼šæ—¥æœŸ_æ—¶é—´ç _S{åœºæ™¯ID}_T{ä»»åŠ¡ID}_æ¨¡å‹å.json
â”‚
â”œâ”€â”€ evaluation_result/        # è¯„ä¼°ç»“æœç›®å½•
â”‚   â””â”€â”€ eval_*.json          # è¯„ä¼°ç»“æœæ–‡ä»¶
â”‚
â”œâ”€â”€ tests/                   # æµ‹è¯•æ–‡ä»¶ç›®å½•
â”œâ”€â”€ docs/                    # æ–‡æ¡£ç›®å½•
â””â”€â”€ *.py                     # é¡¶å±‚æµ‹è¯•è„šæœ¬
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å…‹éš†é¡¹ç›®
git clone <project-url>
cd edu_evaluation

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# åˆå§‹åŒ–é¡¹ç›®é…ç½®
python init.py
```

### 2. é…ç½®åç«¯

åˆ›å»º `.env` æ–‡ä»¶é…ç½®APIå¯†é’¥ï¼š

```bash
# OpenAIé…ç½®
OPENAI_API_KEY=your_openai_api_key
OPENAI_API_BASE=https://api.openai.com/v1

# DeepSeeké…ç½®
DEEPSEEK_API_KEY=your_deepseek_api_key
DEEPSEEK_API_BASE=https://api.deepseek.com

# Ollamaé…ç½®ï¼ˆæœ¬åœ°éƒ¨ç½²ï¼‰
OLLAMA_API_BASE=http://localhost:11434
```

### 3. åŸºæœ¬ä½¿ç”¨

```bash
# è¿è¡Œå•ä¸ªä»»åŠ¡æµ‹è¯•
python test_backends.py

# è¿è¡Œæ‰€æœ‰ä»»åŠ¡
python test_all_tasks.py

# è¯„ä¼°ç»“æœ
python src/evaluator/base.py result_file.json

# æ˜¾ç¤ºåˆ†æ•°
python src/evaluator/show_score.py
```

## ğŸ”„ è¯„ä¼°æµç¨‹

### å®Œæ•´å·¥ä½œæµç¨‹

```
1. ä»»åŠ¡æ‰§è¡Œé˜¶æ®µ
   â”œâ”€â”€ åŠ è½½åœºæ™¯é…ç½® (config/config.yaml)
   â”œâ”€â”€ ç”Ÿæˆæ•™å­¦æç¤ºè¯ (src/prompts/)
   â”œâ”€â”€ è°ƒç”¨LLMç”Ÿæˆæ•™å­¦å†…å®¹ (src/backend/)
   â”œâ”€â”€ å¤„ç†å’Œæ ¼å¼åŒ–å“åº” (src/executor/)
   â””â”€â”€ ä¿å­˜ç»“æœ (results/*.json)

2. è´¨é‡è¯„ä¼°é˜¶æ®µ
   â”œâ”€â”€ è¯»å–ç»“æœæ–‡ä»¶ (results/*.json)
   â”œâ”€â”€ æå–Q&Aå†…å®¹
   â”œâ”€â”€ åŠ è½½è¯„ä¼°æç¤ºè¯ (config/prompts/evaluation_prompts.yaml)
   â”œâ”€â”€ è°ƒç”¨è¯„ä¼°æ¨¡å‹ (evaluation_backend)
   â”œâ”€â”€ è§£æè¯„ä¼°åˆ†æ•°
   â””â”€â”€ ä¿å­˜è¯„ä¼°ç»“æœ (evaluation_result/eval_*.json)

3. ç»“æœå±•ç¤ºé˜¶æ®µ
   â”œâ”€â”€ æå–åˆ†æ•°æ•°æ®
   â”œâ”€â”€ ç»Ÿè®¡åˆ†æ
   â””â”€â”€ æ ¼å¼åŒ–è¾“å‡º
```

### è¯¦ç»†æ‰§è¡Œæ­¥éª¤

#### æ­¥éª¤1ï¼šä»»åŠ¡æ‰§è¡Œ
```python
from src.executor import SingleTurnExecutor
from src.backend import OpenAIBackend
from src.prompts import KnowledgeExplanationPromptGenerator

# åˆå§‹åŒ–ç»„ä»¶
backend = OpenAIBackend()
prompt_generator = KnowledgeExplanationPromptGenerator()
executor = SingleTurnExecutor(backend, prompt_generator, "results")

# æ‰§è¡Œä»»åŠ¡
executor.initialize()
result = executor.execute(task_id=1)
```

#### æ­¥éª¤2ï¼šè´¨é‡è¯„ä¼°
```python
from src.evaluator import evaluate_json

# è¯„ä¼°ç»“æœæ–‡ä»¶
eval_result = evaluate_json(
    result_file_path="results/example.json",
    backend="openai"
)
```

#### æ­¥éª¤3ï¼šç»“æœæŸ¥çœ‹
```python
from src.evaluator import show_score_from_file

# æ˜¾ç¤ºåˆ†æ•°
show_score_from_file("evaluation_result/eval_example.json")
```

## ğŸ“‹ ä¸»è¦åŠŸèƒ½æ¨¡å—

### 1. åç«¯ç³»ç»Ÿ (`src/backend/`)

æ”¯æŒå¤šç§LLMåç«¯ï¼Œç»Ÿä¸€æ¥å£è®¾è®¡ï¼š

- **OpenAI Backend**: æ”¯æŒGPTç³»åˆ—æ¨¡å‹
- **Ollama Backend**: æ”¯æŒæœ¬åœ°éƒ¨ç½²çš„å¼€æºæ¨¡å‹
- **DeepSeek Backend**: æ”¯æŒDeepSeekç³»åˆ—æ¨¡å‹

**ç‰¹æ€§**ï¼š
- ç»Ÿä¸€çš„é…ç½®ç®¡ç†ï¼ˆå‚æ•° > ç¯å¢ƒå˜é‡ > é…ç½®æ–‡ä»¶ > é»˜è®¤å€¼ï¼‰
- è‡ªåŠ¨é‡è¯•å’Œé”™è¯¯å¤„ç†
- æ ‡å‡†åŒ–å“åº”æ ¼å¼

### 2. æ‰§è¡Œå™¨ç³»ç»Ÿ (`src/executor/`)

è´Ÿè´£ä»»åŠ¡çš„å®Œæ•´æ‰§è¡Œæµç¨‹ï¼š

- **SingleTurnExecutor**: å•è½®å¯¹è¯ä»»åŠ¡æ‰§è¡Œ
- **MultiTurnExecutor**: å¤šè½®å¯¹è¯ä»»åŠ¡æ‰§è¡Œï¼ˆå¼€å‘ä¸­ï¼‰

**åŠŸèƒ½**ï¼š
- æç¤ºè¯ç”Ÿæˆ
- æ¨¡å‹è°ƒç”¨
- å“åº”å¤„ç†
- ç»“æœä¿å­˜ï¼ˆS{åœºæ™¯ID}_T{ä»»åŠ¡ID}æ ¼å¼å‘½åï¼‰

### 3. æç¤ºè¯ç³»ç»Ÿ (`src/prompts/`)

åœºæ™¯åŒ–çš„æ•™å­¦æç¤ºè¯ç”Ÿæˆï¼š

- **KnowledgeExplanationPromptGenerator**: çŸ¥è¯†è§£é‡Šåœºæ™¯
- æ”¯æŒæ‰©å±•å…¶ä»–æ•™å­¦åœºæ™¯

### 4. è¯„ä¼°å™¨ç³»ç»Ÿ (`src/evaluator/`)

ä¸“ä¸šçš„æ•™å­¦è´¨é‡è¯„ä¼°ï¼š

- **è‡ªåŠ¨åœºæ™¯è¯†åˆ«**: æ ¹æ®ç»“æœæ–‡ä»¶è‡ªåŠ¨é€‰æ‹©è¯„ä¼°æ ‡å‡†
- **å¤šæ ¼å¼è§£æ**: æ”¯æŒJSONå’Œæ–‡æœ¬æ ¼å¼çš„è¯„ä¼°ç»“æœ
- **ç»Ÿè®¡åˆ†æ**: è‡ªåŠ¨è®¡ç®—æ€»åˆ†ã€å¹³å‡åˆ†ã€æœ€å€¼ç­‰ç»Ÿè®¡æŒ‡æ ‡

### 5. é…ç½®ç³»ç»Ÿ (`config/`)

åˆ†å±‚çš„é…ç½®ç®¡ç†ï¼š

- **config.yaml**: åœºæ™¯å’Œä»»åŠ¡é…ç½®
- **test_backend.yaml**: æµ‹è¯•ç¯å¢ƒåç«¯é…ç½®
- **evaluation_backend.yaml**: è¯„ä¼°ç¯å¢ƒåç«¯é…ç½®
- **evaluation_prompts.yaml**: å„åœºæ™¯è¯„ä¼°æç¤ºè¯

## ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šå•æ¬¡ä»»åŠ¡æ‰§è¡Œå’Œè¯„ä¼°

```bash
# 1. æ‰§è¡ŒçŸ¥è¯†è§£é‡Šä»»åŠ¡
python -c "
from src.executor import SingleTurnExecutor
from src.backend import OpenAIBackend
from src.prompts import KnowledgeExplanationPromptGenerator

backend = OpenAIBackend()
prompt_generator = KnowledgeExplanationPromptGenerator()
executor = SingleTurnExecutor(backend, prompt_generator, 'results')
executor.initialize()
result = executor.execute(1)
print(f'ç»“æœæ–‡ä»¶: {result[\"saved_path\"]}')
"

# 2. è¯„ä¼°ç»“æœ
python src/evaluator/base.py results/latest_file.json

# 3. æŸ¥çœ‹åˆ†æ•°
python src/evaluator/show_score.py
```

### ç¤ºä¾‹2ï¼šæ‰¹é‡å¤„ç†

```bash
# æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
python test_all_tasks.py

# æŸ¥çœ‹æ‰€æœ‰è¯„ä¼°æ–‡ä»¶
python src/evaluator/show_score.py --list

# æŸ¥çœ‹ç‰¹å®šæ–‡ä»¶åˆ†æ•°
python src/evaluator/show_score.py evaluation_result/eval_xxx.json
```

### ç¤ºä¾‹3ï¼šç¨‹åºåŒ–ä½¿ç”¨

```python
from src.evaluator import Evaluator, show_score_from_file

# åˆå§‹åŒ–è¯„ä¼°å™¨
evaluator = Evaluator(backend_config="openai")
evaluator.initialize()

# è¯„ä¼°æ–‡ä»¶
result_path = evaluator.evaluate_file("results/example.json")

# æ˜¾ç¤ºåˆ†æ•°
show_score_from_file(result_path)
```

## ğŸ”§ å¼€å‘æŒ‡å—

### æ·»åŠ æ–°çš„åç«¯

1. ç»§æ‰¿ `Backend` åŸºç±»
2. å®ç° `initialize()` å’Œ `chat()` æ–¹æ³•
3. åœ¨ `src/backend/__init__.py` ä¸­å¯¼å‡º
4. æ›´æ–°é…ç½®æ–‡ä»¶

```python
from src.backend.base import Backend

class NewBackend(Backend):
    def __init__(self, **kwargs):
        super().__init__("new_backend", **kwargs)
    
    def initialize(self) -> bool:
        # åˆå§‹åŒ–é€»è¾‘
        return True
    
    def chat(self, messages, **kwargs):
        # å®ç°å¯¹è¯é€»è¾‘
        pass
```

### æ·»åŠ æ–°çš„åœºæ™¯

1. ç»§æ‰¿ `PromptGenerator` åŸºç±»
2. å®ç°åœºæ™¯ç‰¹å®šçš„æç¤ºè¯ç”Ÿæˆé€»è¾‘
3. åœ¨ `config/config.yaml` ä¸­æ·»åŠ åœºæ™¯é…ç½®
4. åœ¨ `config/prompts/evaluation_prompts.yaml` ä¸­æ·»åŠ è¯„ä¼°æ ‡å‡†

### æ‰©å±•è¯„ä¼°åŠŸèƒ½

è¯„ä¼°å™¨æ”¯æŒå¤šç§åˆ†æ•°æ ¼å¼ï¼š

```json
// ç®€å•æ ¼å¼
{"ç»´åº¦1": 5, "ç»´åº¦2": 4}

// å¤æ‚æ ¼å¼
{"ç»´åº¦1": {"score": 5, "comment": "è¯„ä»·"}}

// åµŒå¥—æ ¼å¼
{"ç±»åˆ«1": {"ç»´åº¦1": {"score": 5}}}
```

## ğŸ“Š æ–‡ä»¶æ ¼å¼è¯´æ˜

### ç»“æœæ–‡ä»¶æ ¼å¼ (`results/*.json`)

```json
{
  "scenario": "åœºæ™¯åç§°",
  "task_id": "ä»»åŠ¡ID",
  "messages": [{"role": "user", "content": "é—®é¢˜"}],
  "raw_response": {"choices": [{"message": {"content": "å›ç­”"}}]},
  "execution_info": {
    "backend": "åç«¯åç§°",
    "model_name": "è¢«æµ‹æ¨¡å‹åç§°",
    "timestamp": "æ‰§è¡Œæ—¶é—´"
  }
}
```

### è¯„ä¼°ç»“æœæ ¼å¼ (`evaluation_result/eval_*.json`)

```json
{
  "original_file": "åŸå§‹ç»“æœæ–‡ä»¶è·¯å¾„",
  "scenario": "åœºæ™¯åç§°",
  "task_id": "ä»»åŠ¡ID",
  "evaluation_backend": "è¯„ä¼°åç«¯",
  "evaluation_model": "è¯„ä¼°æ¨¡å‹",
  "evaluation_response": {"è¯„ä¼°æ¨¡å‹çš„å®Œæ•´å“åº”"},
  "evaluation_timestamp": "è¯„ä¼°æ—¶é—´"
}
```

## ğŸ› ï¸ å¸¸è§é—®é¢˜

### Q: å¦‚ä½•é…ç½®ä¸­æ–‡å­—ä½“æ˜¾ç¤ºï¼Ÿ
A: ç³»ç»Ÿä¼šè‡ªåŠ¨æ£€æµ‹å¯ç”¨çš„ä¸­æ–‡å­—ä½“ã€‚å¦‚æœæ˜¾ç¤ºå¼‚å¸¸ï¼Œå¯ä»¥ï¼š
- macOS: ç³»ç»Ÿè‡ªå¸¦ PingFang SC
- Windows: å®‰è£…å¾®è½¯é›…é»‘
- Linux: `sudo apt-get install fonts-wqy-microhei`

### Q: å¦‚ä½•æ·»åŠ æ–°çš„è¯„ä¼°ç»´åº¦ï¼Ÿ
A: ä¿®æ”¹ `config/prompts/evaluation_prompts.yaml` æ–‡ä»¶ï¼Œæ·»åŠ å¯¹åº”åœºæ™¯çš„è¯„ä¼°æ ‡å‡†ã€‚

### Q: æ”¯æŒå“ªäº›æ¨¡å‹ï¼Ÿ
A: 
- OpenAI: GPT-3.5, GPT-4, GPT-4o ç­‰
- Ollama: Qwen, Llama, Mistral ç­‰æœ¬åœ°æ¨¡å‹
- DeepSeek: DeepSeek-Chat ç­‰

### Q: å¦‚ä½•è‡ªå®šä¹‰è¯„ä¼°æ ‡å‡†ï¼Ÿ
A: åœ¨ `config/prompts/evaluation_prompts.yaml` ä¸­ä¿®æ”¹æˆ–æ·»åŠ åœºæ™¯å¯¹åº”çš„è¯„ä¼°æç¤ºè¯æ¨¡æ¿ã€‚

## ğŸ“ è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·è”ç³»é¡¹ç›®ç»´æŠ¤è€…æˆ–æäº¤ Issueã€‚

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ [MIT License](LICENSE) å¼€æºåè®®ã€‚ 